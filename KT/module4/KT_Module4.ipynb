{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsPuEY7rBpw6"
      },
      "source": [
        "#Module 4:  Classification\n",
        "---\n",
        "\n",
        "Each group should submit to the practical questions on Brightspace.\n",
        "\n",
        "NOTE: For this practical you will need to import the following necessary Python libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbBAyXLEtfxq",
        "outputId": "4f50d3b3-89e1-4968-fb23-b9f2178a0edf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-05 13:39:59--  https://surfdrive.surf.nl/files/index.php/s/ugpzfpKdKWvDh9M/download\n",
            "Resolving surfdrive.surf.nl (surfdrive.surf.nl)... 145.107.8.140, 145.107.56.140, 2001:610:10a:2:0:a11:da7a:5afe, ...\n",
            "Connecting to surfdrive.surf.nl (surfdrive.surf.nl)|145.107.8.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15367 (15K) [text/x-python]\n",
            "Saving to: ‘LST_Functions.py’\n",
            "\n",
            "LST_Functions.py    100%[===================>]  15.01K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-12-05 13:40:00 (159 KB/s) - ‘LST_Functions.py’ saved [15367/15367]\n",
            "\n",
            "--2024-12-05 13:40:00--  https://surfdrive.surf.nl/files/index.php/s/suw9X1gNb7RpWRU/download\n",
            "Resolving surfdrive.surf.nl (surfdrive.surf.nl)... 145.107.8.140, 145.107.56.140, 2001:610:10a:2:0:a11:da7a:5afe, ...\n",
            "Connecting to surfdrive.surf.nl (surfdrive.surf.nl)|145.107.8.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 65191 (64K) [application/octet-stream]\n",
            "Saving to: ‘Genesdata.pkl’\n",
            "\n",
            "Genesdata.pkl       100%[===================>]  63.66K   334KB/s    in 0.2s    \n",
            "\n",
            "2024-12-05 13:40:01 (334 KB/s) - ‘Genesdata.pkl’ saved [65191/65191]\n",
            "\n",
            "--2024-12-05 13:40:01--  https://surfdrive.surf.nl/files/index.php/s/n9VFD2EZTVAGggC/download\n",
            "Resolving surfdrive.surf.nl (surfdrive.surf.nl)... 145.107.8.140, 145.107.56.140, 2001:610:10a:2:0:a11:da7a:5afe, ...\n",
            "Connecting to surfdrive.surf.nl (surfdrive.surf.nl)|145.107.8.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3674 (3.6K) [application/octet-stream]\n",
            "Saving to: ‘cigarsdata.pkl’\n",
            "\n",
            "cigarsdata.pkl      100%[===================>]   3.59K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-12-05 13:40:02 (557 MB/s) - ‘cigarsdata.pkl’ saved [3674/3674]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -nc https://raw.githubusercontent.com/DelftBioinformaticsLab/courses/main/LST/module4/cigarsdata.pkl\n",
        "!wget -nc https://raw.githubusercontent.com/DelftBioinformaticsLab/courses/main/LST/module4/Genesdata.pkl\n",
        "!wget -nc https://raw.githubusercontent.com/DelftBioinformaticsLab/courses/main/LST/LST_Functions.py\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from LST_Functions import plot_decision_boundary, learning_curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPTrHLcIDJ81"
      },
      "source": [
        "##Exercise 1 (Bayesian Classification)\n",
        "\n",
        "The following figure shows the uniform conditional probability density functions of two classes. The first class p(x|ω1) is indicated by a dashed blue line and the second class p(x|ω2) with a solid black line. The two classes have equal priors: p(ω1) = p(ω2) = 1/2.\n",
        "\n",
        "![](https://raw.githubusercontent.com/DelftBioinformaticsLab/courses/main/LST/module4/img/bayesian.png)\n",
        "\n",
        "**1a)** Use Bayes rule to derive the class posterior probabilities of the following objects: x = 3, x = -0.5, x = 1, x = -2. (i.e. for each object x, you want to calculate p(ω1|x) and p(ω2|x)).\n",
        "\n",
        "**1b)** Based on the calculated posterior probabilities, to which class do you assign each object?\n",
        "\n",
        "**1c)** Where do you draw the decision boundary of the Bayes classifier?\n",
        "1d Compute the Bayes error.\n",
        "\n",
        "**1e)** Assume that the class priors are: p(ω1) = 1/3 and p(ω2) =2/3. Recalculate the posterior probabilities for x = 3, x = -0.5, x=1.\n",
        "\n",
        "\n",
        "##Exercise 2 (Bayesian Classification)\n",
        "\n",
        "Based on the two class conditional probabilities of the previous exercise we will generate 200 data points, 100 for each class. The data points generated based on ω1 are uniformly distributed ranging between -1 and 2 (x1). The data points generated based on ω2 are uniformly distributed ranging between 0 and 4 (x2). The following Python code generates this dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGHgrlwfGH-J"
      },
      "outputs": [],
      "source": [
        "# Generate 100 samples for both classes\n",
        "x1 = np.random.uniform(0, 1, 100)*3 -1 # W1\n",
        "x2 = np.random.uniform(0, 1, 100)*4 # W2\n",
        "\n",
        "# Join the samples into one matrix X\n",
        "\n",
        "X = np.hstack((x1, x2))\n",
        "\n",
        "# Create an array of class labels (1 or 2) for the corresponding data points in X\n",
        "y = np.ones(X.shape[0], int)\n",
        "y[100:] = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APj828lwGHhv"
      },
      "source": [
        "**2a)**  we just generated the data based on the class conditional probabilities, we know to which of the two classes each data point actually belongs. However, without that foreknowledge we can also use a Bayes classifier to classify each data point and assign it to either ω1 or ω2.  Complete the Python code below to classify all the objects from the dataset using a Bayes classifier.\n",
        "\n",
        "**2b)** To see how well your Bayes classifier performed, count how many objects from x1 and x2 are misclassified. How does this compare to the Bayes error that you computed in the previous exercise?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxxNvjUrG03Q",
        "outputId": "e1e9e6db-c35a-48bb-a004-e4cc2d95f7de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        }
      ],
      "source": [
        "# For running this section, you need to replace the ??? placeholders with the correct values\n",
        "p_w_1 = 0.5\n",
        "p_w_2 = 0.5\n",
        "p_x_w_1 = 1/3\n",
        "p_x_w_2 = 0.25\n",
        "\n",
        "\n",
        "# An array where the predicted class labels are stored\n",
        "y_predicted = np.zeros(X.shape[0], int)\n",
        "\n",
        "# Loop over all objects and classify them\n",
        "for i in range(0, len(X)):\n",
        "  xi = X[i]\n",
        "\n",
        "  # Calculate P(x_i | w_1)\n",
        "  if (xi >= -1) and (xi <= 2):\n",
        "    p_xi_given_w1 = ??? # Replace the ???\n",
        "  else:\n",
        "    p_xi_given_w1 = 0\n",
        "\n",
        "  # Calculate P(x_i | w_2)\n",
        "  if (xi >= 0) and (xi <= 4):\n",
        "    p_xi_given_w2 = ??? # Replace the ???\n",
        "  else:\n",
        "    p_xi_given_w2 = 0\n",
        "\n",
        "  # Multiply prior with class-conditional\n",
        "  bayes_rule_numerator_w1 = ???  # Replace the ???\n",
        "\n",
        "  bayes_rule_numerator_w2 = ???  # Replace the ???\n",
        "\n",
        "  # Calculate P(x_i) (denominator in bayes rule)\n",
        "  p_xi = ??? # Replace the ???\n",
        "\n",
        "  # Apply the bayes rule to calculate the posterior probability of each class\n",
        "  p_w_1_given_xi = ??? # Replace the ???\n",
        "  p_w_2_given_xi = ??? # Replace the ???\n",
        "\n",
        "  # classify object: w1 or w2\n",
        "  if (p_w_1_given_xi > p_w_2_given_xi):\n",
        "    y_predicted[i] = ??? # Replace the ???\n",
        "  else:\n",
        "    ??? # Replace the ???\n",
        "\n",
        "error_rate = ???\n",
        "print(error_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1IkNVjDIn10"
      },
      "source": [
        "## Exercise 3 (Linear Classifiers)\n",
        "\n",
        "Load the cigars dataset from the *cigarsdata.pkl* file using the Python code below. Create a training and test set with `train_test_split`, with 50% test size. Create a linear discriminant classifier using `LinearDiscriminantAnalysis`, train the classifier using `fit` method. Make a scatter plot of the data and plot the boundary of the classifier using the `plot_decision_boundary` function given in `LST_Functions` (hint: check the function documentation in order to use it properly). Finally, obtain the classifier predictions for the test set using predict method and check the classification error.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7rtpMR5G21c"
      },
      "outputs": [],
      "source": [
        "with open('cigarsdata.pkl', 'rb') as f:\n",
        "  datadict = pickle.load(f)\n",
        "\n",
        "data = datadict['data']\n",
        "labels = datadict['labels']\n",
        "del datadict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiyRAWzvJGZV"
      },
      "source": [
        "##Exercise 4 (Linear Classifiers)\n",
        "\n",
        "Create two interleaving half circles (banana-shaped) dataset with 400 samples, using the following code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpkD68pNJLLq"
      },
      "outputs": [],
      "source": [
        "Banana_shaped = datasets.make_moons(n_samples=400, noise=0.05)\n",
        "data = Banana_shaped[0]\n",
        "labels = Banana_shaped[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUZMAUcXJXyo"
      },
      "source": [
        "**4a)** Repeat the previous exercise using the new data (`LinearDiscriminantAnalysis`, `fit`, `plot_decision_boundary`, and `predict`). Is the linear classifier appropriate for this problem? What is the error rate in this case?\n",
        "\n",
        "**4b)** Scikit-learn (sklearn) library has implemented many different classifiers (see below). Which classifier performs best? (hint: use `plot_decision_boundary` to observe the decision boundary for each classifier, and `train_test_split` to create a training and test set with 50% test size)\n",
        "\n",
        "\n",
        "| Classifier                    | Function name                   |\n",
        "|-------------------------------|---------------------------------|\n",
        "| Linear bayes classifier       | LinearDiscriminantAnalysis()    |\n",
        "| Quadratic bayes classifier    | QuadraticDiscriminantAnalysis() |\n",
        "| k-Nearest neighbor classifier | KNeighborsClassifier()          |\n",
        "| Support Vector Machine        | SVC(probability=True)           |\n",
        "| Nearest mean classifier       | NearestCentroid()               |\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "## Exercise 5 (Training a classifier: effect of training set size)\n",
        "Generate a two-dimensional Gaussian dataset with 20 samples, 2 features. Mean equals to (1,1) and (2,2), and variance equals to 1 and 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7hvCSH9KM6f"
      },
      "outputs": [],
      "source": [
        "Gaussian_shaped = datasets.make_blobs(n_samples=20, n_features=2, centers=[[1, 1], [2, 2]], cluster_std=[1, 2])\n",
        "data = Gaussian_shaped[0]\n",
        "labels =  Gaussian_shaped[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7uAiYXrK2Fx"
      },
      "source": [
        "**5a)** Train the k-Nearest neighbor classifier on the dataset obtained (hint: use KNeighborsClassifier with 3 neighbors, and use fit method to train the classifier)\n",
        "\n",
        "**5b)** Now create a larger dataset with 1000 samples and use it to test the classifier trained in a. What is your error rate now?\n",
        "\n",
        "**5c)** Generate a new Gaussian, two-dimensional dataset with 500 samples.\n",
        "5d Train the k-Nearest neighbor classifier with 3 neighbors on this new set and then test it on the set with 1000 samples in the previous exercise. What is your error rate?\n",
        "\n",
        "**5e)** Can you explain the different error rates obtained? (hint: apply the learning_curve function given in LST_Functions to see the k-Nearest neighbor error on this dataset, using training set of different sizes)\n",
        "\n",
        "## Exercise 6 (Marker gene selection)\n",
        "\n",
        "Load the \"Genesdata.pkl\" file using the code below. This file contains two datasets: (X_train, y_train) and (X_test, y_test). Some genes in these datasets are good markers, while some contain less information about the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPtdR-qzLbVS"
      },
      "outputs": [],
      "source": [
        "# Load the genes dataset\n",
        "with open('Genesdata.pkl', 'rb') as f:\n",
        "  datadict = pickle.load(f)\n",
        "\n",
        "X_train = datadict['X_train']\n",
        "X_test = datadict['X_test']\n",
        "y_train = datadict['y_train']\n",
        "y_test = datadict['y_test']\n",
        "del datadict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1qRdXRhK2E4"
      },
      "source": [
        "**6a)** Employ a criterion such as the t-statistic to evaluate a gene’s predictive power, using the training set. (hint: X_train contains the data, and the class labels are stored in y_train)\n",
        "\n",
        "**6b)** Identify the two best genes by sorting the t-statistic. (hint: remember to look at the absolute value of t-statistic).\n",
        "\n",
        "**6c)** Retain the two top features/genes and train a classifier using the training dataset (X_train, y_train). Then test the trained classifier using the test dataset (X_test, y_test) (hint: using LinearDiscriminantAnalysis, fit and predict functions). What is the error?\n",
        "\n",
        "**6d)** Use plot_decision_boundary to visualize the two selected features for each dataset separately. Are these features the best overall separators for the two classes? Why?\n",
        "\n",
        "\n",
        "##Exercise 7 (Basic Cross-validation)\n",
        "**7a)** Generate a dataset using the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCF4CAY6L6AS"
      },
      "outputs": [],
      "source": [
        "Banana_shaped = datasets.make_moons(n_samples=1000, noise=0.1)\n",
        "data = Banana_shaped[0]\n",
        "labels = Banana_shaped[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hJhRmyPMC3u"
      },
      "source": [
        "**7b)** Use the Python function `StratifiedKFold` to create 3-folds. Use `split` to get training and test fold indices. Split the data into test and training sets using these indices (see code below). For each fold, train three classifiers including linear bayes, SVM and Random Forest and compare their performances. Which one performs better? These classifiers can be trained as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-P9M76oMJmC",
        "outputId": "2e497a72-6779-417f-beeb-ec531225720c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test\n",
            "test\n",
            "test\n"
          ]
        }
      ],
      "source": [
        "# Use one classifier at a time\n",
        "# You need to write the code that completes the for loop.\n",
        "\n",
        "Classifier = LinearDiscriminantAnalysis()\n",
        "Classifier = SVC()\n",
        "Classifier = RandomForestClassifier(n_estimators=3)\n",
        "\n",
        "# Create cross validation and use for loop to cover all folds\n",
        "CV = StratifiedKFold(n_splits=3)\n",
        "for train_ind, test_ind in CV.split(data, labels):\n",
        "  # Complete the for loop."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
